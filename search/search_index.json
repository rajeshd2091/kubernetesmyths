{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Kubernetes Myths","text":"<p>Kubernetes Myths is a deep dive into the most common misconceptions surrounding Kubernetes.  Born out of real-world experience, production incidents, and deep dives into Kubernetes source code, this site aims to bridge the gap between what people assume and how things actually work.</p> Start Reading About the Author"},{"location":"#who-should-read-this-book","title":"Who Should Read This Book?","text":"<p>Whether you're a beginner trying to grasp Kubernetes fundamentals or a seasoned engineer looking to refine your expertise, this book challenges what you \"think\" you know. Each myth is dissected with a backstory, technical analysis, and a step-by-step validation\u2014so this book is for: </p> <ul> <li> <p>Platform Engineers &amp; DevOps Practitioners building scalable Kubernetes infrastructure.</p> </li> <li> <p>Developers who want to understand Kubernetes beyond just kubectl apply.</p> </li> <li> <p>SREs &amp; Architects designing secure and resilient cloud-native systems.</p> </li> <li> <p>Anyone who has hit unexpected Kubernetes behavior and wondered why.</p> </li> </ul> <p>Even if you consider yourself an expert, this book will challenge some of your assumptions. Some of these myths might surprise you.</p>"},{"location":"#how-this-book-is-structured","title":"How This Book is Structured","text":"<p>Each chapter follows a structured approach:</p> <ul> <li> <p>Myth \u2013 The common misconception.</p> </li> <li> <p>Why This Myth Exists \u2013 How and why misinformation spreads.</p> </li> <li> <p>Reality \u2013 What actually happens in Kubernetes.</p> </li> <li> <p>Experiment &amp; Validate \u2013  Real-world evidence to show reality</p> </li> <li> <p>Key Takeaways \u2013 A quick recap of the core lessons.</p> </li> </ul>"},{"location":"about/","title":"\ud83d\udc4b About the Author","text":"<p>Hi, I\u2019m Rajesh Deshpande \u2014 a seasoned DevOps and Platform Engineer with 13+ years of hands-on experience in Kubernetes, Golang, Cloud-Native architecture, and Developer Experience (DevEx).</p>"},{"location":"about/#what-i-do","title":"\ud83c\udfaf What I Do","text":"<ul> <li>I specialize in building secure, scalable platforms using:</li> <li>\ud83d\udc33 Kubernetes</li> <li>\ud83d\udee0\ufe0f Golang (Platform tools, Operators, GitOps automation)</li> <li>\ud83d\udd10 DevSecOps and Zero Trust implementation</li> <li>\u2699\ufe0f Platform Engineering for developer productivity</li> <li>Personal Projects:<ul> <li>Creator of KubernetesMyths.com \u2014 an open knowledge project that debunks Kubernetes myths with real-world experiments and source-code-level analysis.</li> </ul> </li> </ul>"},{"location":"about/#work-highlights","title":"\ud83d\udcbc Work Highlights","text":"<ul> <li>Enabled 10x faster Kubernetes adoption by delivering a scalable, GitOps-driven Orchestrator platform which used to provision 1500+ clusters via self-service and secure automation.</li> <li>Built a self-service Cloud-Native Developer Platform, cutting development time by 80% and infra/tooling costs by 25%.</li> <li>Standardized infra provisioning across the org via a scalable Terraform IaC platform with 100+ reusable modules.</li> <li>Engineered a Jenkins Shared Library framework that powered thousands of pipelines, accelerating deployment velocity and audit compliance</li> <li>Built a secure image lifecycle platform for base images, Jenkins agents, and external artifacts, enabling image signing, vulnerability scanning, and trusted Helm chart acquisition across the organization.</li> <li>Enabled business continuity and operational resilience by helping teams design HA, scalable, cloud-native systems with built-in disaster recovery across clouds.</li> <li>Enabled distributed observability and service mesh integration, improving diagnostics, performance monitoring, and cross-service policy governance</li> </ul>"},{"location":"about/#presence","title":"\ud83d\udcd8 Presence","text":"<ul> <li>\ud83d\udcbb GitHub: rajeshd2091</li> <li>\ud83e\uddf5 LinkedIn: linkedin.com/in/rajeshdeshpande02</li> </ul>"},{"location":"about/#open-to-opportunities","title":"\ud83d\ude80 Open to Opportunities","text":"<p>I\u2019m open to:</p> <ul> <li>\ud83d\udd27 Senior Platform Engineering / DevOps roles</li> <li>\ud83e\udde0 Consulting on Kubernetes, Platform, or Supply Chain Security</li> <li>\ud83d\udce6 Building internal developer platforms</li> </ul> <p>\ud83d\udce8 Let\u2019s connect on LinkedIn or reach me directly at rajeshd2090@gmail.com</p> <p>\u201cTools come and go, but platform thinking, clarity, and curiosity scale forever.\u201d</p>"},{"location":"architecture_myths/Control_Plane_Nodes_Don%E2%80%99t_Need_a_Container_Runtime/","title":"Myth:  Control Plane Nodes Don\u2019t Need a Container Runtime","text":"<p>You set up your control plane node, confident that everything is configured correctly. Next, you run <code>kubeadm init</code>, expecting a smooth setup\u2014but it fails instantly!</p> <p>The error? <code>no container runtime detected.</code></p> <p>That\u2019s strange. Isn\u2019t the container runtime needed only on worker nodes to run application workloads? You double-check your configuration, thinking you might have missed a step. But the error persists.</p> <p>What\u2019s going on here? Why is Kubernetes complaining about a missing CRI on the control plane?</p>"},{"location":"architecture_myths/Control_Plane_Nodes_Don%E2%80%99t_Need_a_Container_Runtime/#why-does-this-myth-exist","title":"Why Does This Myth Exist?","text":"<ol> <li>Over-Simplified Architecture Explanations \u2013 Many Kubernetes learning resources describe the architecture in a rigid way: Control plane runs API Server, etcd, Scheduler, and Controller Manager, while worker nodes run Kubelet, CRI, and Kube-Proxy. This oversimplification leads to misconceptions.</li> <li>Misinterpretation of Control Plane Responsibilities \u2013 Since control plane components are often seen as \"management-only,\" people assume they don\u2019t rely on container runtimes, networking components, or Kubelet.</li> <li>Managed Kubernetes Abstraction \u2013 Many managed Kubernetes services abstract away the control plane, making engineers believe that only worker nodes require components like CRI, Kubelet, or CNI.</li> <li>Historical Understanding of Kubernetes \u2013 Earlier Kubernetes documentation and tutorials emphasized worker nodes as the place where \"real workloads\" run, reinforcing the belief that key infrastructure components don\u2019t function similarly on control plane nodes.</li> </ol>"},{"location":"architecture_myths/Control_Plane_Nodes_Don%E2%80%99t_Need_a_Container_Runtime/#the-reality","title":"The Reality","text":"<p>CRI is Essential on Control Plane Nodes\u2014But with Exceptions:</p> <ul> <li> <p>When control plane components (like API Server, Controller Manager, and etcd) run as static pods, a Container Runtime Interface (CRI) is required because Kubelet manages them as containers.</p> </li> <li> <p>However, in some Kubernetes setups (e.g., certain managed services or custom-built clusters), control plane components run as systemd services instead of static pods. In such cases, the control plane can function without a CRI.</p> </li> <li> <p>Despite this exception, most Kubernetes distributions rely on static pods, making CRI a fundamental requirement for both control plane and worker nodes.</p> </li> <li> <p>Even if the control plane runs without a CRI, worker nodes still require it to manage application workloads.</p> </li> </ul>"},{"location":"architecture_myths/Control_Plane_Nodes_Don%E2%80%99t_Need_a_Container_Runtime/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Check if the Control Plane Uses CRI Run: <pre><code>kubectl get pods -n kube-system\n</code></pre></p> <p>Expected Output (If CRI is Present): <pre><code>NAME                               READY   STATUS    RESTARTS   AGE\nkube-apiserver-control-plane       1/1     Running   0          5m\netcd-control-plane                 1/1     Running   0          5m\n</code></pre> If these pods are missing or stuck in <code>ContainerCreating</code>, it likely means CRI is missing.</p> <p>Step 2: Verify CRI Connectivity Run: <pre><code>crictl ps\n</code></pre> Expected Output (If CRI is Missing): <pre><code>E0205 10:22:34.123456   1234 runtime.go:300] no runtime configured\n</code></pre> Confirming that the control plane needs a CRI just like worker nodes.</p>"},{"location":"architecture_myths/Control_Plane_Nodes_Don%E2%80%99t_Need_a_Container_Runtime/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>CRI is required on all nodes where Kubernetes components run as containers, including control plane nodes.</li> <li>Control plane components (like API Server, etcd, and Controller Manager) run as static pods in most Kubernetes setups, requiring a container runtime.</li> <li>Some Kubernetes distributions run control plane components as systemd services, in which case CRI is not needed on control plane nodes.</li> <li>Without a CRI, kubeadm cannot initialize the cluster, and critical control plane services won\u2019t start if they rely on containers.</li> <li>Understanding CRI\u2019s role helps in troubleshooting missing services and failed cluster initialization issues.</li> </ul>"},{"location":"architecture_myths/Kubelet_is_Exclusive_to_Worker_Nodes/","title":"Myth: Kubelet is Exclusive to Worker Nodes","text":"<p>You SSH into a control plane node, expecting to see only control plane components like the API server, controller manager, and scheduler running. But wait\u2014why is Kubelet there? Wasn't it supposed to run only on worker nodes?</p> <p>You double-check your understanding: worker nodes handle workloads, and Kubelet is responsible for managing pods on those nodes. Control plane nodes, on the other hand, orchestrate everything but don\u2019t run regular workloads. So, why does <code>ps aux | grep kubelet</code> show Kubelet actively running on a control plane node?</p> <p>You start to wonder\u2014if control plane nodes aren\u2019t supposed to run application pods, does Kubelet even serve a purpose here?</p>"},{"location":"architecture_myths/Kubelet_is_Exclusive_to_Worker_Nodes/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>Worker nodes are responsible for running workloads, leading many to believe Kubelet is exclusive to them.</li> <li>Control plane nodes don\u2019t schedule regular workloads, making Kubelet\u2019s presence less noticeable.</li> <li>Many tutorials and diagrams oversimplify by associating Kubelet only with worker nodes.</li> </ol>"},{"location":"architecture_myths/Kubelet_is_Exclusive_to_Worker_Nodes/#the-reality","title":"The Reality","text":"<p>Kubelet runs on all nodes in a typical Kubernetes cluster, including control plane nodes. However, its role differs:</p> <ul> <li> <p>On worker nodes: Kubelet registers the node and manages pod execution.</p> </li> <li> <p>On control plane nodes (when using static pods): Kubelet ensures control plane components like the API server, scheduler, and controller manager run correctly.</p> </li> </ul> <p>Exception: Some Kubernetes distributions (e.g., certain air-gapped or enterprise setups) run control plane components as systemd services instead of static pods. In such cases, Kubelet may not be needed on control plane nodes. However, this is uncommon in most Kubernetes distributions.</p>"},{"location":"architecture_myths/Kubelet_is_Exclusive_to_Worker_Nodes/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Check Kubelet Running on Control Plane Nodes <pre><code>ssh &lt;control-plane-node&gt;\nps aux | grep kubelet\n</code></pre> If Kubelet is running, you\u2019ll see its process listed.</p> <p>Step 2: Verify the Kubelet service status (For Most Kubernetes Setups) <pre><code>ssh &lt;control-plane-node&gt;\nsystemctl status kubelet --no-pager\n</code></pre> If active, Kubelet is running on the control plane node.</p> <p>Step 3: Confirm Kubelet's Role in Managing Static Pods Since Kubelet manages static pods on control plane nodes, you can check for static pod manifests: <pre><code>ls /etc/kubernetes/manifests/\n</code></pre> If you see files like kube-apiserver.yaml and etcd.yaml, Kubelet is responsible for running control plane components as static pods.</p>"},{"location":"architecture_myths/Kubelet_is_Exclusive_to_Worker_Nodes/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Kubelet runs on all worker nodes and is commonly present on control plane nodes.</li> <li>On worker nodes, Kubelet registers the node and manages workload execution.</li> <li>On control plane nodes, Kubelet typically manages static pods for core components unless systemd-based services are used.</li> <li>Understanding Kubelet\u2019s role is essential for troubleshooting and maintaining cluster stability.</li> </ul>"},{"location":"architecture_myths/Kubernetes_Clusters_Can%27t_Function_Without_Kube-Proxy/","title":"Myth: Kubernetes Clusters Can't Function Without Kube-Proxy","text":"<p>You deploy a Kubernetes cluster and start checking the usual system components. API server? Running. Controller manager? Running. Kube-Proxy? Wait\u2026 it's missing! You double-check the namespace, logs, and even the deployment\u2014nothing. But surprisingly, your pods and services are still communicating just fine. How is this possible? Isn't Kube-Proxy essential for cluster networking?</p>"},{"location":"architecture_myths/Kubernetes_Clusters_Can%27t_Function_Without_Kube-Proxy/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>Kube-Proxy is enabled by default in most Kubernetes distributions, making it appear mandatory.</li> <li>Traditional Kubernetes networking relies on iptables or IPVS, which are managed by Kube-Proxy, reinforcing its perceived necessity.</li> <li>Many engineers are unaware of eBPF-based networking solutions, such as Cilium, which can entirely replace Kube-Proxy while improving performance and scalability.</li> </ol>"},{"location":"architecture_myths/Kubernetes_Clusters_Can%27t_Function_Without_Kube-Proxy/#reality","title":"Reality","text":"<ul> <li>Kube-Proxy is a default networking component in Kubernetes, but it is not mandatory for cluster functionality. Kubernetes networking is designed to support multiple implementations, and modern solutions can replace Kube-Proxy entirely.</li> <li>Newer networking models, such as eBPF-based CNIs (e.g., Cilium, Calico eBPF, and Katran), bypass the need for Kube-Proxy by directly handling packet processing within the kernel. These solutions not only eliminate the dependency on iptables/IPVS but also improve performance, scalability, and security by reducing latency and enabling fine-grained traffic control.</li> <li>Many high-performance Kubernetes clusters\u2014especially those optimized for large-scale workloads\u2014choose to disable Kube-Proxy in favor of these more efficient networking alternatives.</li> </ul>"},{"location":"architecture_myths/Kubernetes_Clusters_Can%27t_Function_Without_Kube-Proxy/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Disable Kube-Proxy Scale down Kube-Proxy to remove it from the cluster: <pre><code>kubectl -n kube-system scale deployment kube-proxy --replicas=0\n</code></pre></p> <p>Step 2: Install Cilium Without Kube-Proxy Use Cilium as a replacement, enabling eBPF-based networking: <pre><code>helm install cilium cilium/cilium --set kubeProxyReplacement=strict\n</code></pre></p> <p>Step 3: Deploy a Service and Test Connectivity Create an Nginx pod and expose it as a service: <pre><code>kubectl run nginx --image=nginx --port=80 --expose\n</code></pre></p> <p>Now, check if the service works without Kube-Proxy: <pre><code>kubectl run test-pod --image=busybox --restart=Never --rm -it -- wget -qO- nginx\n</code></pre> If successful, you\u2019ll see the Nginx welcome page, proving that Kubernetes networking works without Kube-Proxy.</p>"},{"location":"architecture_myths/Kubernetes_Clusters_Can%27t_Function_Without_Kube-Proxy/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Kube-Proxy is NOT mandatory\u2014Kubernetes networking can function without it.</li> <li>eBPF-based CNIs (Cilium, Calico eBPF, etc.) can fully replace Kube-Proxy, offering better performance and scalability.</li> <li>Choosing the right networking approach is crucial\u2014understanding Kubernetes networking beyond Kube-Proxy helps optimize cluster efficiency, security, and observability.</li> </ul>"},{"location":"architecture_myths/Kubernetes_Networking_Works_Fine_Without_a_CNI_Plugin/","title":"Myth: Kubernetes Networking Works Fine Without a CNI Plugin","text":"<p>You set up a Production Kubernetes cluster, deploy some pods, and\u2026 nothing. They can't talk to each other, You search Stack Overflow, try restarting pods, but nothing works, Finally, you realize networking is broken, and some pods are stuck in \"ContainerCreating\" state. You check the logs and see: <pre><code>NetworkPluginNotReady: CNI plugin not initialized\n</code></pre> So, What might be wrong? </p>"},{"location":"architecture_myths/Kubernetes_Networking_Works_Fine_Without_a_CNI_Plugin/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>Local environments create an illusion \u2013 Some Kubernetes distributions (like Minikube, k3s, and kind) include built-in networking, making it seem like a CNI is optional. Since these setups \"just work\" out of the box, users may not realize a CNI is missing in a standard cluster.</li> <li>Kubernetes does not bundle a default CNI \u2013 Unlike other components (like the scheduler or API server), Kubernetes does not ship with a preinstalled CNI. This can mislead users into thinking networking is an add-on rather than a fundamental requirement.</li> <li>Single-node clusters work without a CNI \u2013 On a single-node cluster, all pods can share the host\u2019s networking stack. This makes basic pod communication seem functional, masking the fact that a proper CNI is required for multi-node networking, pod IP assignment, and policies.</li> </ol>"},{"location":"architecture_myths/Kubernetes_Networking_Works_Fine_Without_a_CNI_Plugin/#the-reality","title":"The Reality:","text":"<p>A CNI (Container Network Interface) plugin is essential for Production Grade Kubernetes networking\u2014without it, core networking functionalities break.</p> <ul> <li> <p>Pod-to-pod communication depends on CNI \u2013 Kubernetes does not manage networking by itself; it relies on a CNI to assign IP addresses and enable connectivity between pods, especially across nodes.</p> </li> <li> <p>Multi-node clusters require a CNI \u2013 Without a CNI, pods on different nodes cannot communicate because Kubernetes does not provide an internal networking layer.</p> </li> <li> <p>Critical Kubernetes features won\u2019t work \u2013 Services, network policies, and pod IP management all rely on a CNI. Without it, pods may remain stuck in the <code>ContainerCreating</code> state, and basic networking between workloads will fail.</p> </li> </ul> <p>Even though local setups like k3s and Minikube may seem to work without a visible CNI,this is because they embed lightweight networking solutions. In a standard Kubernetes cluster, a CNI is mandatory for networking to function.</p>"},{"location":"architecture_myths/Kubernetes_Networking_Works_Fine_Without_a_CNI_Plugin/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Check Pod Status Without a CNI If a Kubernetes cluster is deployed without a CNI, pods will be stuck in the ContainerCreating state. Run: <pre><code>kubectl get pods -A\n</code></pre></p> <p>Example output: <pre><code>NAMESPACE     NAME                          READY   STATUS              RESTARTS   AGE\nkube-system   coredns-78fcd69978-2v7        0/1     ContainerCreating   0          10m\nkube-system   coredns-78fcd69978-s7jxp      0/1     ContainerCreating   0          10m\n</code></pre></p> <p>Step 2: Check Node Conditions Inspect the node description to verify networking errors: <pre><code>kubectl describe node &lt;node-name&gt;\n</code></pre></p> <p>You'll likely see an error like: <pre><code>NetworkPluginNotReady: network plugin is not ready: cni plugin not initialized\n</code></pre></p> <p>Step 3: Verify Missing Network Interfaces Kubernetes assigns pod IPs through the CNI. If no CNI is installed, pods won\u2019t receive IPs: <pre><code>kubectl get pods -o wide\n</code></pre> If the IP column is empty or missing, it confirms that pod networking is broken due to a missing CNI.</p>"},{"location":"architecture_myths/Kubernetes_Networking_Works_Fine_Without_a_CNI_Plugin/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Kubernetes delegates networking to a CNI, making it a required component.</li> <li>Without a CNI, pods won\u2019t get IPs, and multi-node clusters won\u2019t work.</li> <li>Some local environments provide built-in networking, making CNIs seem optional..</li> <li>Always verify a CNI is installed to ensure Kubernetes networking works as expected.</li> </ul>"},{"location":"architecture_myths/overview/","title":"Kubernetes Architecture Myths","text":"<p>Kubernetes architecture is often perceived as a black box\u2014with terms like \"control plane,\" \"kubelet,\" and \"scheduler\" thrown around without deep clarity. This section tackles myths rooted in these components, where surface-level understanding leads to poor design choices and debugging frustrations.</p> <p>From misunderstanding how the scheduler works to misattributing responsibilities between components like the kube-apiserver and kubelet, these myths expose critical gaps in architectural knowledge.</p> <p>By busting these myths, we aim to deepen your mental model of Kubernetes and help you design more reliable, secure, and efficient clusters.</p>"},{"location":"architecture_myths/overview/#myths","title":"Myths","text":"<ul> <li>Kubelet is Exclusive to Worker Nodes </li> <li>Kubernetes Clusters Can't Function Without Kube-Proxy </li> <li>Kubernetes Networking Works Fine Without a CNI Plugin</li> <li>Control Plane Nodes Don\u2019t Need a Container Runtime</li> <li>A Kubernetes Cluster Must Have Three Control Plane Nodes</li> </ul>"},{"location":"networking_myths/ClusterIP_Service_is_Only_for_Internal_Traffic/","title":"Myth: ClusterIP Service is Only for Internal Traffic","text":"<p>You\u2019ve just configured a ClusterIP service to expose one of your pods internally in your Kubernetes cluster. It seems like a straightforward setup\u2014after all, ClusterIP services are meant to route traffic between pods within the cluster, right? So, you assume that it\u2019s all about internal communication.</p> <p>But then, you decide to test it a bit more. You try accessing the service using a curl command from a different pod within the same cluster and\u2026 it works perfectly. That\u2019s expected, right? You start thinking that ClusterIP is only for internal use. But curiosity leads you to try something else\u2014accessing the service from outside the cluster.</p> <p>To your surprise, it works! You use a load balancer or a proxy to access the service, and your request is successfully routed to the ClusterIP service. Wait\u2014how can this be? ClusterIP should only be for internal traffic, right?</p>"},{"location":"networking_myths/ClusterIP_Service_is_Only_for_Internal_Traffic/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>Kubernetes documentation emphasizes ClusterIP as an \"internal\" service type, leading people to overlook its role in external communication.</li> <li>NodePort and LoadBalancer are explicitly designed for external access, making it seem like ClusterIP plays no role in exposing services.</li> <li>Developers often misunderstand how Kubernetes routes traffic, missing the fact that external service types still forward traffic through ClusterIP.</li> </ol>"},{"location":"networking_myths/ClusterIP_Service_is_Only_for_Internal_Traffic/#the-reality","title":"The Reality","text":"<p>Yes, a pure ClusterIP service is internal, but every Kubernetes service\u2014whether NodePort or LoadBalancer\u2014relies on ClusterIP behind the scenes. Even when traffic comes from outside the cluster, it still flows through ClusterIP before reaching the pods.</p>"},{"location":"networking_myths/ClusterIP_Service_is_Only_for_Internal_Traffic/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Create a LoadBalancer Service Apply the following YAML to create a LoadBalancer service:</p> <p><pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-loadbalancer-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n  type: LoadBalancer\n</code></pre> Step 2: Verify the Created Service Run the following command to check the details of the service: <pre><code>kubectl get svc my-loadbalancer-service -o wide\n</code></pre></p> <p>Expected Output: <pre><code>NAME                      TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)       AGE\nmy-loadbalancer-service   LoadBalancer   10.96.0.150  &lt;pending&gt;     80:32000/TCP  5s\n</code></pre> Notice the CLUSTER-IP field\u2014Kubernetes has automatically assigned a ClusterIP to the LoadBalancer service.</p> <p>Step 3: Confirm That ClusterIP Exists Even though the service is a LoadBalancer, it still functions like a ClusterIP service inside the cluster. You can verify this by running: <pre><code>kubectl get endpoints my-loadbalancer-service\n</code></pre> Expected Output: <pre><code>NAME                      ENDPOINTS           AGE\nmy-loadbalancer-service   192.168.1.100:8080  5s\n</code></pre></p> <p>This proves that traffic is routed through the ClusterIP before reaching the pods.</p>"},{"location":"networking_myths/ClusterIP_Service_is_Only_for_Internal_Traffic/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>ClusterIP is not just for internal traffic; it is essential for external services like NodePort and LoadBalancer.</li> <li>Every Kubernetes service type ultimately forwards traffic through ClusterIP before reaching the pods.</li> <li>Misunderstanding ClusterIP\u2019s role can lead to unnecessary service type changes and wasted troubleshooting efforts.</li> </ul>"},{"location":"networking_myths/ClusterIP_Services_Always_Use_Round-Robin_Load_Balancing/","title":"Myth: ClusterIP Service Always Use Round-Robin Load Balancing","text":"<p>Have you ever assumed that your pods are getting equal traffic? Many engineers believe Kubernetes distributes traffic evenly across pods in a strict round-robin manner. But if you actually monitor request distribution, you\u2019ll notice something surprising\u2014some pods receive more traffic than others. Is Kubernetes failing at load balancing? Not really. It turns out that the default behavior isn't what most people expect.</p>"},{"location":"networking_myths/ClusterIP_Services_Always_Use_Round-Robin_Load_Balancing/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>People assume Kubernetes works like traditional load balancers, which often default to round-robin.</li> <li>The presence of multiple backend pods creates an expectation of even traffic distribution.</li> <li>The internal working of kube-proxy is often overlooked</li> </ol>"},{"location":"networking_myths/ClusterIP_Services_Always_Use_Round-Robin_Load_Balancing/#the-reality","title":"The Reality","text":"<p>The default iptables-based kube-proxy does not use round-robin. Instead, it relies on random probability-based selection when forwarding traffic to backend pods.</p>"},{"location":"networking_myths/ClusterIP_Services_Always_Use_Round-Robin_Load_Balancing/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Create a ClusterIP Service with Multiple Pods Apply the following YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: test-clusterip\nspec:\n  selector:\n    app: test-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: test-app\n  template:\n    metadata:\n      labels:\n        app: test-app\n    spec:\n      containers:\n        - name: app\n          image: nginx\n          ports:\n            - containerPort: 8080\n</code></pre></p> <p>Step 1: Send Requests and Observe the Pattern Run multiple requests from a test pod: <pre><code>kubectl run test --rm -it --image=busybox -- /bin/sh\n</code></pre></p> <p>Inside the pod, execute: <pre><code>while true; do wget -qO- http://test-clusterip.default.svc.cluster.local; sleep 1; done\n</code></pre></p>"},{"location":"networking_myths/ClusterIP_Services_Always_Use_Round-Robin_Load_Balancing/#todo-include-kiali-image","title":"TODO - Include Kiali image","text":"<p>You'll notice that some pods receive more traffic than others\u2014proving that the selection is random and not round-robin.</p>"},{"location":"networking_myths/ClusterIP_Services_Always_Use_Round-Robin_Load_Balancing/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>ClusterIP does not guarantee round-robin load balancing\u2014default iptables mode uses random selection.</li> <li>IPVS mode can enable true round-robin scheduling in Kubernetes.</li> <li>External load balancers provide more fine-grained control if round-robin is essential.</li> </ul>"},{"location":"networking_myths/kube-proxy_assign_IP_address_to_Pods/","title":"Myth: kube-proxy assign IP address to Pods","text":"<p>A common belief among Kubernetes users is that kube-proxy is responsible for assigning IP addresses to pods. After all, it manages networking rules and enables communication between services\u2014so it must be the component handling pod IPs, right?</p>"},{"location":"networking_myths/kube-proxy_assign_IP_address_to_Pods/#why-does-this-myth-exist","title":"Why Does This Myth Exist?","text":"<ol> <li>Kube-proxy\u2019s prominent role in networking: Kube-proxy is integral to service-level networking in Kubernetes. Since it manages traffic routing between services and uses iptables, many assume it also handles pod-level networking, including IP assignment.</li> <li>Confusion with iptables: Kube-proxy\u2019s involvement with iptables and traffic forwarding may create the impression that it manages all networking tasks, including assigning IPs to pods.</li> <li>Misunderstanding of Kubernetes components: Newcomers to Kubernetes often see kube-proxy running in the cluster and mistakenly believe it is responsible for all networking-related operations, including pod IP allocation.</li> </ol>"},{"location":"networking_myths/kube-proxy_assign_IP_address_to_Pods/#the-reality","title":"The Reality","text":"<ul> <li>Pod IP allocation is the responsibility of the CNI (Container Network Interface) plugin, not kube-proxy. The CNI is responsible for assigning network interfaces and IP addresses to each pod when they are created.</li> <li>Kube-proxy\u2019s role is service-level networking: It ensures traffic reaches the right pods by managing network rules for services, but it does not assign IP addresses to the pods themselves.</li> <li>Pods depend on the CNI: If there\u2019s no CNI plugin installed or running, Kubernetes pods won\u2019t receive an IP address, even if kube-proxy is operational.</li> </ul>"},{"location":"networking_myths/kube-proxy_assign_IP_address_to_Pods/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>To verify that kube-proxy is not responsible for pod IP allocation, follow these steps:</p> <p>Step 1: Check the IP of an existing pod before disabling kube-proxy</p> <p>Run the following command to view the pod\u2019s details:</p> <pre><code>kubectl get pods -o wide\n</code></pre> <p>Example output: <pre><code>NAME          READY STATUS    IP          NODE\nnginx-abc123  1/1   Running  10.244.1.2  worker-node-1\n</code></pre></p> <p>Step 2: Disable kube-proxy temporarily</p> <p>Scale down the kube-proxy deployment to zero replicas:</p> <pre><code>kubectl scale deployment/kube-proxy -n kube-system --replicas=0\n</code></pre> <p>Step 3: Create a new pod and check if it receives an IP address</p> <p>Launch a new pod and Then, verify the new pod's details::</p> <pre><code>kubectl run test-pod --image=nginx --restart=Never\nkubectl get pods -o wide\n</code></pre> <p>You\u2019ll observe that the new pod still receives an IP address, even with kube-proxy disabled, which demonstrates that CNI, not kube-proxy, is responsible for assigning pod IPs.</p>"},{"location":"networking_myths/kube-proxy_assign_IP_address_to_Pods/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Kube-proxy does not assign pod IPs: Pod IP allocation is handled by the CNI plugin, not kube-proxy.</li> <li>Kube-proxy\u2019s role is service routing: It manages the traffic routing for services, but not the network configuration or IP assignment for pods.</li> <li>Without a CNI plugin, pods will fail to receive IP addresses, regardless of whether kube-proxy is running.</li> <li>Always ensure a CNI plugin is installed when setting up Kubernetes to guarantee proper networking functionality for pods.</li> </ul>"},{"location":"networking_myths/kubectl_port-forward_svc_sends_traffic_to_a_service/","title":"Myth: 'kubectl port-forward svc' sends traffic to a service","text":"<p>Many engineers assume that running: <pre><code>kubectl port-forward svc/my-service 8080:80\n</code></pre> routes traffic through the Service to its backend Pods\u2014just like a real client request would. But if you analyze the traffic flow, you'll notice something surprising\u2014the Service is completely bypassed!</p>"},{"location":"networking_myths/kubectl_port-forward_svc_sends_traffic_to_a_service/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>The command uses <code>svc/my-service</code>, which makes it seem like traffic will flow through the Service.</li> <li>Since Services load balance traffic, it's natural to assume <code>port-forward</code> does the same.</li> <li>The actual forwarding logic is not well-documented, leading to confusion.</li> </ol>"},{"location":"networking_myths/kubectl_port-forward_svc_sends_traffic_to_a_service/#the-reality","title":"The Reality","text":"<p>Even if you specify a Service in the <code>kubectl port-forward</code> command, the traffic never reaches the Service. Instead, Kubernetes picks a single Pod behind the Service and forwards traffic directly to that Pod\u2014bypassing the Service entirely.</p>"},{"location":"networking_myths/kubectl_port-forward_svc_sends_traffic_to_a_service/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Create a Service with Multiple Pods Apply the following YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: productpage\nspec:\n  selector:\n    app: productpage\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: productpage\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: productpage\n  template:\n    metadata:\n      labels:\n        app: productpage\n    spec:\n      containers:\n        - name: app\n          image: nginx\n          ports:\n            - containerPort: 8080\n</code></pre> Step 2: Run kubectl port-forward and Check Traffic Flow Forward traffic using: <pre><code>kubectl port-forward svc/productpage 8080:80\n</code></pre> Then check which Pod is actually receiving traffic:</p>"},{"location":"networking_myths/kubectl_port-forward_svc_sends_traffic_to_a_service/#todo-include-kiali-image","title":"TODO - Include Kiali image","text":"<p>You'll notice no traffic hits the Service itself, confirming that requests go directly to a Pod.</p>"},{"location":"networking_myths/kubectl_port-forward_svc_sends_traffic_to_a_service/#key-takeaways","title":"Key Takeaways","text":"<ul> <li><code>kubectl port-forward</code> does not send traffic through the Service\u2014it picks one Pod and forwards directly.</li> <li>No load balancing: Traffic is not distributed among multiple Pods.</li> <li>No failover: If the chosen Pod crashes, the connection is lost.</li> <li>Not a true service test: If you want to simulate real client traffic, use Ingress, LoadBalancer, or NodePort.</li> </ul>"},{"location":"networking_myths/overview/","title":"Kubernetes Networking Myths","text":"<p>Networking in Kubernetes often feels like magic\u2014until it breaks.</p> <p>This section uncovers common misconceptions about how networking works inside a cluster. From Pod-to-Pod communication and Services to DNS, Network Policies, and CNI plugins, these myths reveal gaps that lead to confusing outages, misconfigured policies, and insecure setups.</p> <p>Whether you're debugging traffic issues or designing multi-tenant clusters, busting these myths will sharpen your understanding of Kubernetes networking internals.</p>"},{"location":"networking_myths/overview/#myths","title":"Myths","text":"<ul> <li>kube-proxy assign IP address to Pods </li> <li>ClusterIP Service is Only for Internal Traffic </li> <li>ClusterIP Service Always Use Round-Robin Load Balancing</li> <li>'kubectl port-forward svc' sends traffic to a service</li> </ul>"},{"location":"security_myths/Kubernetes_Namespaces_Provide_Complete_Isolation/","title":"Myth: Kubernetes Namespaces Provide Complete Isolation","text":"<p>Many teams assume that creating separate namespaces guarantees strong isolation between workloads, preventing them from affecting each other. But this belief can lead to critical security oversights.</p> <p>A developer once confidently deployed production and staging workloads in separate namespaces on the same cluster, believing they were fully isolated. Later, a misconfigured role binding allowed an engineer to access production resources from the staging namespace\u2014resulting in an unintended outage.</p>"},{"location":"security_myths/Kubernetes_Namespaces_Provide_Complete_Isolation/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<p>This myth persists because namespaces feel like isolated environments in practice. They offer separate dashboards, configs, and resource groupings \u2014 which gives the illusion of sandboxing. Here\u2019s why people are misled:</p> <ul> <li> <p>Visual &amp; Logical Separation:  Tools like kubectl, dashboards, and YAML files naturally group resources by namespace, leading engineers to assume there's also runtime separation.</p> </li> <li> <p>RBAC Defaults Are Namespace-Scoped: Since many RBAC policies are written per-namespace, teams assume the underlying access and execution contexts are automatically isolated.</p> </li> <li> <p>Cloud Vendors Encourage Namespaces for Multi-Tenancy: Many best practices and tutorials recommend using namespaces for separating teams or environments \u2014 without emphasizing the limitations of that approach.</p> </li> <li> <p>No Immediate Breakage in Small Projects: In development or staging setups, cross-namespace issues are rare \u2014 so teams wrongly assume this behavior holds in production environments too.</p> </li> <li> <p>Terminology Confusion: The term \u201cnamespace\u201d is borrowed from programming (e.g., Java/C++), where it usually does imply hard boundaries. This contributes to the false sense of isolation.</p> </li> </ul>"},{"location":"security_myths/Kubernetes_Namespaces_Provide_Complete_Isolation/#the-reality","title":"The Reality:","text":"<p>Namespaces are a convenience feature for organizing workloads, not a security mechanism. Here\u2019s what they actually provide:</p> <ul> <li> <p>Logical separation of Kubernetes objects (pods, services, secrets, etc.).</p> </li> <li> <p>Quota enforcement to limit resource consumption per namespace.</p> </li> <li> <p>RBAC (Role-Based Access Control) to define access policies, but not by default.</p> </li> </ul> <p>But they do not:</p> <ul> <li> <p>Prevent cross-namespace network traffic.</p> </li> <li> <p>Guarantee CPU/memory isolation across namespaces.</p> </li> <li> <p>Restrict access without proper RBAC and Network Policies.</p> </li> </ul>"},{"location":"security_myths/Kubernetes_Namespaces_Provide_Complete_Isolation/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1. Create two namespaces:</p> <pre><code>kubectl create namespace dev  \nkubectl create namespace prod  \n</code></pre> <p>Step 2. Deploy a test pod in each:</p> <pre><code>kubectl run nginx-dev --image=nginx -n dev  \nkubectl run nginx-prod --image=nginx -n prod  \n</code></pre> <p>Setp 3. From the dev namespace, try resolving a service in prod:</p> <pre><code>kubectl exec -n dev nginx-dev -- nslookup nginx-prod.prod.svc.cluster.local \n</code></pre> <p>Surprise! The pod in dev can still resolve services in prod, proving that namespaces don\u2019t block network communication by default.</p>"},{"location":"security_myths/Kubernetes_Namespaces_Provide_Complete_Isolation/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Namespaces help organize resources but do not enforce strong isolation.</li> <li>Security and access control must be explicitly defined using RBAC and Network Policies.</li> <li>For strict separation, consider multi-cluster architectures.</li> </ul>"},{"location":"security_myths/Kubernetes_Service_Accounts_Pull_Container_Images/","title":"Myth: Kubernetes Service Accounts Pull Container Images","text":"<p>During internal security audits and team discussions, several engineers assumed that ServiceAccounts were used to authenticate with container registries. One developer even tried granting additional RBAC permissions to a ServiceAccount in an attempt to fix an image-pull failure from a private registry. The issue persisted, and only later did the team realize that the ServiceAccount had no role in image retrieval at all. The failure was caused by missing imagePullSecrets, not ServiceAccount permissions.</p>"},{"location":"security_myths/Kubernetes_Service_Accounts_Pull_Container_Images/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<p>This misconception commonly arises because of two overlapping facts:</p> <ul> <li> <p>ServiceAccounts can be linked to imagePullSecrets. When engineers attach registry credentials to a ServiceAccount, it creates an impression that the ServiceAccount is the entity responsible for authenticating and pulling images.</p> </li> <li> <p>ServiceAccount tokens are automatically mounted into Pods. Since these tokens provide authentication to the Kubernetes API server, developers assume they might also be used for authenticating with container registries.</p> </li> </ul> <p>Both features involve authentication, but they serve different purposes, leading to widespread confusion.</p>"},{"location":"security_myths/Kubernetes_Service_Accounts_Pull_Container_Images/#the-reality","title":"The Reality:","text":"<p>A Kubernetes ServiceAccount has absolutely no role in pulling container images.</p> <p>Image pulling is performed entirely by the container runtime (containerd, CRI-O, or Docker) running on the worker node.</p> <p>Here is the actual sequence:</p> <ol> <li> <p>A Pod is scheduled onto a node.</p> </li> <li> <p>Kubelet verifies whether the required image exists locally.</p> </li> <li> <p>If the image is missing, Kubelet instructs the container runtime to pull it using the Container Runtime Interface (CRI).</p> </li> <li> <p>The container runtime performs registry authentication using the credentials provided in imagePullSecrets.</p> </li> <li> <p>Once authenticated, the runtime downloads, stores, and unpacks the image before starting the container.</p> </li> </ol> <p>The ServiceAccount is not consulted at any point in this workflow. It is solely used for Pod-to-API-server authentication.</p>"},{"location":"security_myths/Kubernetes_Service_Accounts_Pull_Container_Images/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1. Create a Pod using a private image without imagePullSecrets</p> <p>Use a private image and do not specify any imagePullSecrets:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-no-creds\nspec:\n  containers:\n    - name: app\n      image: my-private-registry.com/app:v1\n</code></pre> <p>Observe the Pod state:</p> <pre><code>kubectl describe pod test-no-creds\n</code></pre> <p>You will see events such as: <pre><code>Failed to pull image \"my-private-registry.com/app:v1\": authentication required\n</code></pre> Notice that RBAC permissions on the ServiceAccount do not affect this result.</p> <p>Setp 2. Attach imagePullSecrets directly to the Pod</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-with-creds\nspec:\n  containers:\n    - name: app\n      image: my-private-registry.com/app:v1\n  imagePullSecrets:\n    - name: my-registry-secret\n</code></pre> <p>The Pod will now successfully pull the image and start, proving that registry authentication depends on imagePullSecrets, not ServiceAccount permissions.</p> <p>Setp 3. Attach imagePullSecrets to a ServiceAccount</p> <p><pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-sa\nimagePullSecrets:\n  - name: my-registry-secret\n</code></pre> and then:</p> <p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-using-sa\nspec:\n  serviceAccountName: my-sa\n  containers:\n    - name: app\n      image: my-private-registry.com/app:v1\n</code></pre> The Pod succeeds\u2014but the authentication still happens exclusively through the imagePullSecret, not through the ServiceAccount token.</p> <p>The ServiceAccount merely references the secret; it never interacts with the container registry.</p>"},{"location":"security_myths/Kubernetes_Service_Accounts_Pull_Container_Images/#key-takeaways","title":"Key Takeaways","text":"<ul> <li> <p>ServiceAccounts do not pull container images.</p> </li> <li> <p>The container runtime on the node is solely responsible for pulling images.</p> </li> <li> <p>Kubelet only instructs the runtime; it does not authenticate with registries.</p> </li> <li> <p>Private images require imagePullSecrets, not ServiceAccount permissions.</p> </li> <li> <p>ServiceAccounts authenticate Pods to the Kubernetes API server, not to container registries.</p> </li> </ul>"},{"location":"security_myths/overview/","title":"Kubernetes Security Myths","text":"<p>Kubernetes security is often misunderstood, not because the platform is insecure, but because its design principles differ from traditional infrastructure and application security models. Many teams assume Kubernetes automatically provides strong isolation, network safety, secure defaults, or hardened workloads. In reality, Kubernetes only offers building blocks\u2014misinterpreting these building blocks leads to configuration gaps, risky deployments, and a false sense of security.</p> <p>This section examines the most common security-related misconceptions that lead to real-world vulnerabilities. Each myth is broken down using practical examples, technical explanations, and reproducible experiments. The goal is to clarify how Kubernetes security actually works, where responsibilities lie, and what assumptions commonly break secure-by-default thinking.</p> <p>By understanding and correcting these myths, engineers can avoid design flaws, reduce attack surfaces, and build security controls aligned with how Kubernetes truly operates.</p>"},{"location":"security_myths/overview/#myths","title":"Myths","text":"<ul> <li>Kubernetes Namespaces Provide Complete Isolation</li> <li>Kubernetes Service Accounts Pull Container Images</li> <li>Kubernetes Always Injects the Default Service Account Into Every Pod</li> </ul>"},{"location":"storage_myths/Kubernetes_Cluster_Can_Have_Only_One_Default_StorageClass/","title":"Myth: Kubernetes Cluster Can Have Only One Default StorageClass","text":"<p>You're defining a new StorageClass and set: <pre><code>metadata:\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"\n</code></pre> You assume Kubernetes will reject or warn if another default already exists. After all, \"default\" means only one, right?</p> <p>So you apply multiple StorageClass resources with the same default annotation \u2014 and nothing happens. No error. No warning.</p> <p>You run: <pre><code>kubectl get sc -o custom-columns=NAME:.metadata.name,DEFAULT:.metadata.annotations.storageclass\\.kubernetes\\.io/is-default-class\n</code></pre> And you see two or more default classes marked \"true\".</p>"},{"location":"storage_myths/Kubernetes_Cluster_Can_Have_Only_One_Default_StorageClass/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ul> <li>The term \u201cdefault\u201d creates the impression that only one can exist.</li> <li>Most tutorials and cluster setups only define one, reinforcing this assumption.</li> <li>The lack of enforcement is not widely documented.</li> <li>Engineers expect the system to prevent ambiguous behavior\u2014but Kubernetes allows it.</li> </ul>"},{"location":"storage_myths/Kubernetes_Cluster_Can_Have_Only_One_Default_StorageClass/#the-reality","title":"The Reality","text":"<p>You can create multiple default StorageClasses in a Kubernetes cluster.  The  <code>\"storageclass.kubernetes.io/is-default-class\": \"true\"</code> annotation is not enforced by Kubernetes \u2014 it's simply a marker.</p> <ul> <li>When a PVC omits storageClassName, the system looks for a StorageClass with that annotation.</li> <li>If more than one exists,  Kubernetes uses the most recently created default StorageClass.</li> <li>Kubernetes does not raise any warning or error when multiple defaults exist.</li> <li>It's up to cluster admins to ensure only one default exists for consistent PVC provisioning.</li> </ul>"},{"location":"storage_myths/Kubernetes_Cluster_Can_Have_Only_One_Default_StorageClass/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Create Two Default StorageClasses</p> <p><pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fast-default\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"\nprovisioner: kubernetes.io/no-provisioner\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: slow-default\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"\nprovisioner: kubernetes.io/no-provisioner\n</code></pre> Step 2: Apply <pre><code>kubectl apply -f dual-default-sc.yaml\n</code></pre> Step 3: List All Defaults <pre><code>kubectl get sc -o custom-columns=NAME:.metadata.name,DEFAULT:.metadata.annotations.storageclass\\.kubernetes\\.io/is-default-class\n</code></pre> You\u2019ll see both fast-default and slow-default marked as default. No error. No warning.</p>"},{"location":"storage_myths/Kubernetes_Cluster_Can_Have_Only_One_Default_StorageClass/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Kubernetes allows multiple default StorageClasses without any warning.</li> <li>The \u201cdefault\u201d is an annotation, not a rule.</li> <li>If more than one default exists, PVCs without storageClassName will use most recently created StorageClass.</li> <li>Always ensure only one default StorageClass exists to avoid confusion.</li> </ul>"},{"location":"storage_myths/Kubernetes_PersistentVolumeClaim_Can_Be_Resized/","title":"Myth: Kubernetes <code>PersistentVolumeClaim</code> Can Be Resized","text":"<p>During a design discussion, a team member confidently stated that \u201cPVCs in Kubernetes are fully resizable \u2014 you can increase or decrease storage whenever you want.\u201d This assumption influenced their storage planning and automated cleanup strategy.</p> <p>The misunderstanding became clear only when they attempted to shrink a PVC and discovered that Kubernetes does not support reducing its size, leading to unnecessary downtime and manual recovery work.</p>"},{"location":"storage_myths/Kubernetes_PersistentVolumeClaim_Can_Be_Resized/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<p>This misconception commonly arises because:</p> <ul> <li> <p>Kubernetes documentation and almost all blogs use the term \u201cresize,\u201d which people assume includes both expansion and shrinking.</p> </li> <li> <p>Storage systems outside Kubernetes (such as cloud block storage) often support dynamic resizing, which leads developers to expect the same flexibility at the PVC level.</p> </li> <li> <p>Kubernetes allows expansion of PVCs starting from v1.11+, so users incorrectly generalize that resizing must also include shrinking.</p> </li> </ul> <p>Because of these factors, the term \u201cresize\u201d is frequently interpreted incorrectly.</p>"},{"location":"storage_myths/Kubernetes_PersistentVolumeClaim_Can_Be_Resized/#the-reality","title":"The Reality","text":"<p>Kubernetes PVCs can be expanded but cannot be shrunk. The resize operation is unidirectional.</p> <ul> <li> <p>Kubernetes allows increasing PVC size when the StorageClass supports volume expansion.</p> </li> <li> <p>Kubernetes does not support reducing a PVC\u2019s requested storage.</p> </li> <li> <p>Shrinking storage poses significant data integrity risks (such as truncation), so Kubernetes explicitly disallows it.</p> </li> <li> <p>The underlying storage provider might support shrink operations, but Kubernetes does not expose this capability to users.</p> </li> <li> <p>If a smaller volume is required, the only safe approach is to create a new, smaller PVC and migrate data manually.</p> </li> </ul> <p>In summary, calling PVCs \u201cresizable\u201d is misleading because Kubernetes supports only expansion \u2014 not shrinkage.</p>"},{"location":"storage_myths/Kubernetes_PersistentVolumeClaim_Can_Be_Resized/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Create a PVC</p> <p><pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-demo\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: standard\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> Apply it:</p> <p><pre><code>kubectl apply -f pvc.yaml\n</code></pre> Verify:</p> <pre><code>kubectl get pvc pvc-demo\n</code></pre> <p>Step 2: Expand the PVC</p> <p>Edit the PVC:</p> <pre><code>kubectl edit pvc pvc-demo\n</code></pre> <p>Change:</p> <pre><code>resources:\n  requests:\n    storage: 10Gi\n</code></pre> <p>Kubernetes will allow this change</p> <pre><code>kubectl get pvc pvc-demo\n</code></pre> <p>Status shows: <pre><code>pvc-demo   Bound   10Gi   ...\n</code></pre></p> <p>Step 3: Attempt to Shrink the PVC</p> <p>Edit the PVC again:</p> <pre><code>kubectl edit pvc pvc-demo\n</code></pre> <p>Change: <pre><code>resources:\n  requests:\n    storage: 2Gi\n</code></pre></p> <p>Kubernetes rejects the change with an error similar to:</p> <p><pre><code>persistentvolumeclaims \"pvc-demo\" is invalid: \nspec.resources.requests.storage: Forbidden: field can not be less than status.capacity\n</code></pre> The PVC remains at 10Gi, confirming that shrink operations are blocked.</p>"},{"location":"storage_myths/Kubernetes_PersistentVolumeClaim_Can_Be_Resized/#key-takeaways","title":"Key Takeaways","text":"<ul> <li> <p>A PVC in Kubernetes can only be expanded, never shrunk.</p> </li> <li> <p>The StorageClass must have <code>allowVolumeExpansion=true</code> to support expansion.</p> </li> <li> <p>Kubernetes explicitly prevents shrink operations to protect data from corruption.</p> </li> <li> <p>Shrinking a PVC requires creating a new, smaller PVC and migrating data manually.</p> </li> <li> <p>The term \u201cresize\u201d is misleading; technically, Kubernetes supports only volume expansion.</p> </li> </ul>"},{"location":"storage_myths/ReadWriteOnce_mode_allows_only_a_single_Pod_to_access_the_volume/","title":"Myth: <code>ReadWriteOnce</code> mode allows only a single Pod to access the volume","text":"<p>During one of my interviews, the panel asked me, \u201cWhy does Kubernetes restrict RWO volumes to a single Pod?\u201d I confidently answered, \u201cBecause only one Pod can mount it \u2014 that\u2019s what RWO means.\u201d The interviewer smiled and said, \u201cAre you sure? What if those Pods are on the same node?\u201d</p> <p>That question triggered me to revisit the actual behavior, and I realized how widely this myth is believed.</p>"},{"location":"storage_myths/ReadWriteOnce_mode_allows_only_a_single_Pod_to_access_the_volume/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ul> <li>For many years, Kubernetes only had ReadWriteOnce (RWO), and there was no access mode that enforced single-Pod exclusivity until ReadWriteOncePod (RWOP) was introduced.</li> <li>Most developers interpret RWO literally \u2014 \u201conly one Pod can write.\u201d</li> <li>Documentation often states \u201cRWO = mounted by a single node,\u201d which many people convert in their head to \u201csingle Pod.\u201d</li> <li>In everyday clusters, Pods with the same volume usually get scheduled on different nodes, making it look like Kubernetes enforces a single-Pod restriction.</li> <li>Because of this pattern, people assume that RWO inherently blocks multiple Pods.</li> </ul>"},{"location":"storage_myths/ReadWriteOnce_mode_allows_only_a_single_Pod_to_access_the_volume/#the-reality","title":"The Reality","text":"<p>ReadWriteOnce (RWO) means the volume can be mounted by only a single node at a time, not a single Pod.</p> <p>If multiple Pods run on the same node, they can simultaneously mount the same RWO PersistentVolumeClaim.</p> <p>To avoid this confusion, Kubernetes also introduced a more strict access mode:</p> <p>ReadWriteOncePod (RWOP) \u2013 the volume can be mounted as read-write by only a single Pod, even if multiple Pods are on the same node. This mode enforces the \u201cexactly one Pod only\u201d behavior that many people mistakenly assume RWO provides.</p> <p>Key points:</p> <ul> <li>RWO = single node restriction, multiple Pods allowed on that node.</li> <li>RWOP = single Pod restriction, even on the same node.</li> <li>RWOP is stricter and guarantees true single-writer semantics at Pod level.</li> <li>If Pods are scheduled to different nodes, both RWO and RWOP will trigger the same multi-attach error.</li> <li>RWOP makes the access rule explicit, solving the long-standing confusion around RWO.</li> </ul>"},{"location":"storage_myths/ReadWriteOnce_mode_allows_only_a_single_Pod_to_access_the_volume/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step1: Create a PVC with RWO</p> <p><pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: rwo-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre> Step 2: Create two Pods that both mount the same PVC <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-a\nspec:\n  volumes:\n    - name: data\n      persistentVolumeClaim:\n        claimName: rwo-pvc\n  containers:\n    - name: app\n      image: alpine\n      command: [\"sh\", \"-c\", \"sleep 3600\"]\n      volumeMounts:\n        - mountPath: \"/data\"\n          name: data\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-b\nspec:\n  volumes:\n    - name: data\n      persistentVolumeClaim:\n        claimName: rwo-pvc\n  containers:\n    - name: app\n      image: alpine\n      command: [\"sh\", \"-c\", \"sleep 3600\"]\n      volumeMounts:\n        - mountPath: \"/data\"\n          name: data\n</code></pre> Step 3: Force both Pods on the same node Add this to both Pod specs if needed: <pre><code>nodeSelector:\n  kubernetes.io/hostname: &lt;same-node-name&gt;\n</code></pre> Step 4: Observe</p> <p>Both Pods will start successfully and mount the same RWO volume.</p> <p>Step 5: Now move one Pod to another node</p> <p>Change the nodeSelector of pod-b to a different node.</p> <p>Result: The second Pod will fail to start with an error similar to: <pre><code>You\u2019ll see both **fast-default** and **slow-default** marked as default. No error. No warning.\n</code></pre></p> <p>This confirms that the lock is at the node level, not the Pod level.</p>"},{"location":"storage_myths/ReadWriteOnce_mode_allows_only_a_single_Pod_to_access_the_volume/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>ReadWriteOnce restricts a volume to a single node, not a single Pod.</li> <li>Multiple Pods on the same node can mount the same RWO PVC simultaneously.</li> <li>Multi-Pod failures occur only when Pods land on different nodes, causing a block device multi-attach conflict.</li> <li>Understanding this helps design workloads like StatefulSets, Jobs, and shared-process Pods more effectively.</li> <li>RWO is a node-level access mode, not a Pod-level limitation.</li> </ul>"},{"location":"storage_myths/overview/","title":"Kubernetes Storage Myths","text":"<p>In Kubernetes, compute may steal the spotlight \u2014 but persistent storage is what truly powers workloads.</p> <p>From databases to logs, caches to backups, storage is a foundational layer that often gets abstracted away, leading to confusion, poor design decisions, and long-term operational pain.</p> <p>Unlike traditional VMs or monolithic servers, Kubernetes operates in a dynamic, ephemeral environment:</p> <ul> <li> <p>Pods are rescheduled.</p> </li> <li> <p>Nodes come and go.</p> </li> <li> <p>Volumes need to move with workloads.</p> </li> </ul> <p>This demands storage that\u2019s dynamic, portable, and declarative \u2014 but also reliable, consistent, and secure.</p> <p>Kubernetes addresses this through:</p> <ul> <li> <p>PersistentVolumes (PV) and PersistentVolumeClaims (PVC)</p> </li> <li> <p>StorageClasses</p> </li> <li> <p>Dynamic provisioning</p> </li> <li> <p>CSI (Container Storage Interface)</p> </li> </ul> <p>But understanding these components is only half the battle. The rest is unlearning the myths.</p>"},{"location":"storage_myths/overview/#myths","title":"Myths","text":"<ul> <li>K8s Cluster can have only one default Storage Class </li> <li><code>ReadWriteOnce</code> mode allows only a single Pod to access the volume</li> <li>Kubernetes <code>PersistentVolumeClaim</code> Can Be Resized</li> </ul>"},{"location":"workload_myths/Complete_application_can_be_rolled_back_in_Kubernetes/","title":"Myth: Complete application can be rolled back in Kubernetes","text":"<p>A team deployed a new version of their microservices-based application in Kubernetes. Soon after, they noticed critical issues and decided to roll back using: <pre><code>kubectl rollout undo deployment my-app\n</code></pre> They expected everything to return to normal instantly. Instead, they faced database inconsistencies, mismatched API versions, and persistent storage corruption. Their rollback didn\u2019t restore the full application state\u2014only the container images reverted.</p>"},{"location":"workload_myths/Complete_application_can_be_rolled_back_in_Kubernetes/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<p>This myth persists because many assume Kubernetes rollbacks work like traditional application restore mechanisms. The key reasons behind this misconception are:</p> <ol> <li> <p>Rollback Command Appears Comprehensive \u2013 The <code>kubectl rollout undo</code> command suggests it restores everything, but it only affects Deployments, not databases, ConfigMaps, or storage.</p> </li> <li> <p>Stateless vs. Stateful Confusion \u2013 Stateless apps can be rolled back easily, but stateful workloads (databases, message queues, persistent volumes) require additional rollback strategies.</p> </li> <li> <p>Lack of Awareness About External Dependencies \u2013 Applications in Kubernetes often depend on external databases, cloud storage, or third-party services, which are unaffected by a simple rollback.</p> </li> <li> <p>Expectation from Traditional Monolithic Rollbacks \u2013 In legacy systems, rolling back a single instance often meant restoring the full application state, whereas Kubernetes primarily focuses on container lifecycle management.</p> </li> <li> <p>Kubernetes Default Behavior is Not Full Recovery \u2013 Kubernetes prioritizes high availability and rolling updates but does not track or revert external dependencies automatically.</p> </li> </ol> <p>This misunderstanding leads teams to believe that a simple Kubernetes rollback will restore the entire application state when, in reality, it only reverts container versions and pod specifications.</p>"},{"location":"workload_myths/Complete_application_can_be_rolled_back_in_Kubernetes/#the-reality","title":"The Reality:","text":"<p>Rolling back a deployment does not mean your entire application is restored. Kubernetes can revert container images and pod specifications, but:</p> <ul> <li> <p>Databases Require Explicit Rollbacks \u2013 Schema migrations must be reversible or manually handled.</p> </li> <li> <p>Persistent Volumes Retain Data \u2013 Rolling back an app doesn\u2019t roll back stored data unless snapshots or backups exist.</p> </li> <li> <p>Configuration Drift Can Occur \u2013 Changes in ConfigMaps or Secrets won\u2019t automatically revert unless managed separately.</p> </li> <li> <p>Interdependent Services Can Break \u2013 A rollback might mismatch versions between microservices, causing API incompatibilities.</p> </li> </ul>"},{"location":"workload_myths/Complete_application_can_be_rolled_back_in_Kubernetes/#experiment-validate","title":"Experiment &amp; Validate","text":"<ol> <li>Deploy an app and apply a schema migration <pre><code>kubectl create deployment my-app --image=my-app:v1  \nkubectl apply -f database-migration.yaml  # Applies a schema change  \n</code></pre></li> <li>Roll back the deployment <pre><code>kubectl rollout undo deployment my-app  \n</code></pre> Result: The app\u2019s code is rolled back, but the database remains on the new schema, potentially breaking compatibility.</li> </ol>"},{"location":"workload_myths/Complete_application_can_be_rolled_back_in_Kubernetes/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Kubernetes rollbacks only revert Deployments, not the entire application state.</li> <li>Persistent data, ConfigMaps, and external services are unaffected by a rollback.</li> <li>Database schema changes, inter-service dependencies, and persistent storage require separate rollback strategies.</li> </ul>"},{"location":"workload_myths/DaemonSet_always_schedule_pods_on_all_nodes/","title":"Myth: DaemonSet always schedule pods on all nodes.","text":"<p>Expect DaemonSet to Run on Every Node? Not So Fast! Many assume that a DaemonSet automatically schedules a Pod on every node in the cluster. While this is generally true, there are several cases where DaemonSet does not schedule Pods on all nodes.</p>"},{"location":"workload_myths/DaemonSet_always_schedule_pods_on_all_nodes/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>The default behavior of a DaemonSet is to run a Pod on each node, leading to the assumption that it applies universally.</li> <li>Many Kubernetes tutorials demonstrate cluster-wide scheduling without mentioning constraints.</li> <li>The impact of node taints, selectors, and affinity rules is often overlooked.</li> </ol>"},{"location":"workload_myths/DaemonSet_always_schedule_pods_on_all_nodes/#the-reality","title":"The Reality:","text":"<p>DaemonSet Scheduling Is Conditional. It does not always schedule Pods on every node. Various factors can prevent or control where Pods are scheduled: - Taints and Tolerations \u2013 Nodes with taints will reject DaemonSet Pods unless explicitly tolerated. - Node Selectors \u2013 DaemonSet Pods are only scheduled on nodes matching the specified labels. - Affinity and Anti-Affinity Rules \u2013 Custom scheduling rules can limit DaemonSet Pod placement.</p>"},{"location":"workload_myths/DaemonSet_always_schedule_pods_on_all_nodes/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Let\u2019s see a example where DaemonSet does NOT schedule Pods on all nodes: Node Selector: DaemonSet Pods Only on Labeled Node <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: my-daemonset\nspec:\n  template:\n    spec:\n      nodeSelector:\n        custom-role: worker-node\n      containers:\n      - name: my-daemon\n        image: my-app:v2\n</code></pre> Here, Pods will only run on nodes labeled <code>custom-role=worker-node</code>, skipping all others.</p>"},{"location":"workload_myths/DaemonSet_always_schedule_pods_on_all_nodes/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>DaemonSets do not always schedule on all nodes\u2014constraints like taints, selectors, and affinity rules affect placement.</li> <li>By default, they schedule on worker nodes but require tolerations for control-plane nodes.</li> <li>Understanding scheduling logic is crucial to ensure Pods run where they are needed.</li> </ul>"},{"location":"workload_myths/Deployment_Supports_All_Pod_Restart_Policies/","title":"Myth: Deployment Supports All Pod Restart Policies","text":"<p>Can You Use Any Restart Policy in a Deployment?  Many believe that Deployments can be used with any Kubernetes Pod restart policy. The assumption is that you can define <code>Always</code>, <code>OnFailure</code>, or <code>Never</code> as the restartPolicy within a Deployment\u2019s Pod spec.But that's not true.</p>"},{"location":"workload_myths/Deployment_Supports_All_Pod_Restart_Policies/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>Pods support all three restart policies (<code>Always</code>, <code>OnFailure</code>, <code>Never</code>), leading to the assumption that Deployments do as well.</li> <li>Users often assume Pod behavior is the same inside different controllers like Deployments, DaemonSets, or Jobs.</li> <li>The Kubernetes API allows specifying other restart policies in Pod definitions, but Deployments override them!</li> </ol>"},{"location":"workload_myths/Deployment_Supports_All_Pod_Restart_Policies/#the-reality","title":"The Reality:","text":"<p>Deployment = <code>restartPolicy: Always</code> Only! In Kubernetes, Deployments are designed to manage long-running applications. That means: - If a Pod fails, Kubernetes restarts it. - If a node crashes, a new Pod is scheduled on another node. - Any restart policy other than Always is rejected!</p>"},{"location":"workload_myths/Deployment_Supports_All_Pod_Restart_Policies/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Let\u2019s try using <code>restartPolicy: OnFailure</code> inside a Deployment and see what happens:</p> <p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: test-app\n  template:\n    metadata:\n      labels:\n        app: test-app\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: test-container\n        image: busybox\n        command: [\"sh\", \"-c\", \"echo Hello &amp;&amp; exit 1\"]\n</code></pre> What Happens? Deployment rejects <code>OnFailure</code> and throw an error! <pre><code>The Deployment \"my-deployment\" is invalid: spec.template.spec.restartPolicy: Unsupported value: \"OnFailure\": supported values: \"Always\"\n</code></pre> Kubernetes rejects the Deployment, as it does not allow restart policies other than <code>Always</code>.</p>"},{"location":"workload_myths/Deployment_Supports_All_Pod_Restart_Policies/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Deployments only support <code>restartPolicy: Always</code>\u2014other policies are rejected.</li> <li>Use Jobs or CronJobs for <code>OnFailure</code> or <code>Never</code> restart behavior.</li> <li>Understanding controller-specific behavior prevents misconfigurations.</li> </ul>"},{"location":"workload_myths/K8s_Injects_Svc_Info_Into_A_Pods_Env_Var/","title":"Myth: Kubernetes always injects information about Services into a Pod","text":"<p>I was attending a free Kubernetes webinar where the speaker was walking through service discovery patterns. At one point, he confidently said:</p> <p><code>Don\u2019t worry, we can just read the service IP from the pod\u2019s environment variables \u2014 Kubernetes puts it there by default.</code></p> <p>That statement sounded comforting \u2014 especially for teams coming from legacy environments where service discovery needs extra tooling.</p>"},{"location":"workload_myths/K8s_Injects_Svc_Info_Into_A_Pods_Env_Var/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li> <p>In early Kubernetes versions, service environment variables were one of the primary methods for service discovery.</p> </li> <li> <p>Documentation and tutorials often reference <code>MY_SERVICE_SERVICE_HOST</code>, <code>MY_SERVICE_PORT</code>, etc. \u2014 and developers assumed they always exist.</p> </li> </ol>"},{"location":"workload_myths/K8s_Injects_Svc_Info_Into_A_Pods_Env_Var/#the-reality","title":"The Reality:","text":"<p>Kubernetes only injects environment variables for services that already exist at the time the pod is created.</p> <ul> <li> <p>If a Service is created after a Pod starts \u2192 No env variables are injected.</p> </li> <li> <p>If <code>enableServiceLinks = false</code>, Service information is not injected into Pod as environment variables</p> </li> <li> <p>If a Pod is restarted after the Service exists \u2192 Then env vars appear.</p> </li> <li> <p>Headless Services (ClusterIP: None) do not get env vars injected.</p> </li> </ul> <p>For dynamic environments, DNS-based service discovery (..svc.cluster.local) is the reliable method \u2014 not env vars."},{"location":"workload_myths/K8s_Injects_Svc_Info_Into_A_Pods_Env_Var/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step1: Create a pod before the service</p> <p><code>kubectl run test-pod --image=busybox -it --restart=Never -- sh</code></p> <p>Inside the pod, try to check the environment</p> <p><code>env | grep MYAPP</code></p> <p>Step2: Now create the service</p> <p><code>kubectl expose pod test-pod --port=8080 --name=myapp</code></p> <p>Back to pod \u2014 still no env vars injected</p> <p>Step3:  Delete and recreate the pod</p> <pre><code>kubectl delete pod test-pod\n\nkubectl run test-pod --image=busybox -it --restart=Never -- sh\n</code></pre> <p>Now run:</p> <p><code>`env | grep MYAPP</code></p> <p>You\u2019ll now see:</p> <pre><code>MYAPP_SERVICE_PORT=8080\nMYAPP_SERVICE_HOST=10.97.212.84 #Some random IP\n</code></pre> <p>Also try with a headless service and observe that no env vars get injected.</p>"},{"location":"workload_myths/K8s_Injects_Svc_Info_Into_A_Pods_Env_Var/#key-takeaways","title":"Key Takeaways","text":"<ul> <li> <p>Kubernetes injects service env vars only at pod creation time, not dynamically.</p> </li> <li> <p>Pods won\u2019t auto-update their env vars if a Service is created later.</p> </li> <li> <p>Headless services do not inject env vars at all.</p> </li> <li> <p>Always use Cluster DNS names for robust service discovery.</p> </li> </ul>"},{"location":"workload_myths/Kubernetes_automatically_roll_back_failed_deployment/","title":"Myth: Kubernetes automatically roll back failed Deployment","text":"<p>A team deployed a new version of their application, expecting Kubernetes to roll it back automatically if something went wrong. Unfortunately, the deployment failed, but instead of rolling back, Kubernetes left it in a bad state. The team was surprised\u2014wasn't Kubernetes supposed to handle this automatically?</p>"},{"location":"workload_myths/Kubernetes_automatically_roll_back_failed_deployment/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<p>This misunderstanding comes from:</p> <ol> <li> <p>Expectation from Traditional Systems \u2013 Some deployment tools provide automatic rollback mechanisms, leading users to assume Kubernetes does the same.</p> </li> <li> <p>Confusion with Health Checks \u2013 Kubernetes can stop bad deployments using readiness and liveness probes, but stopping an update is different from rolling back.</p> </li> <li> <p>Rollback Feature Misinterpretation \u2013 Since <code>kubectl rollout undo</code> exists, people assume Kubernetes triggers it automatically.</p> </li> <li> <p>Expectation from CI/CD Pipelines \u2013 Some CI/CD tools like ArgoCD and Spinnaker offer auto-rollbacks, making users think Kubernetes itself provides this feature.</p> </li> <li> <p>Misunderstanding <code>progressDeadlineSeconds</code> \u2013 Many believe setting <code>progressDeadlineSeconds</code> triggers an automatic rollback, but it only marks the deployment as failed.</p> </li> </ol>"},{"location":"workload_myths/Kubernetes_automatically_roll_back_failed_deployment/#the-reality","title":"The Reality","text":"<p>Kubernetes does not automatically roll back a failed Deployment. While it detects failures, it requires manual intervention to revert to a previous version.Here\u2019s what actually happens:</p> <ul> <li> <p>If a Deployment update fails, Kubernetes pauses further updates but does not roll back.</p> </li> <li> <p>You must manually trigger a rollback using <code>kubectl rollout undo deployment &lt;name&gt;</code>.</p> </li> <li> <p>Kubernetes tracks revisions, but rollbacks only work for Deployments\u2014they do not revert ConfigMaps, Secrets, or Persistent Volumes.</p> </li> <li> <p><code>progressDeadlineSeconds</code> plays a crucial role \u2013 If a rollout does not complete within the configured <code>progressDeadlineSeconds</code>, the Deployment is marked as failed, but Kubernetes does not roll it back automatically. Instead, the rollout gets paused, requiring manual intervention.</p> </li> </ul>"},{"location":"workload_myths/Kubernetes_automatically_roll_back_failed_deployment/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Create a Working Deployment <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 2\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  progressDeadlineSeconds: 30\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:latest\n          ports:\n            - containerPort: 80\n</code></pre></p> <p>Apply the Deployment: <pre><code>kubectl apply -f nginx-deployment.yaml\nkubectl rollout status deployment/nginx-deployment\n</code></pre> This should show a successful rollout.</p> <p>Step 2: Trigger a Failed Deployment Now, let's break the deployment by using an invalid container image. <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 2\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  progressDeadlineSeconds: 30\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:doesnotexist  # Invalid image\n          ports:\n            - containerPort: 80\n</code></pre> Apply the broken Deployment: <pre><code>kubectl apply -f nginx-deployment.yaml\n</code></pre></p> <p>Check the rollout status: <pre><code>kubectl rollout status deployment/nginx-deployment --timeout=60s\nkubectl rollout status deployment/nginx-deployment --timeout=60s\n</code></pre></p> <p>After <code>progressDeadlineSeconds</code> (30 seconds in this case), Kubernetes will mark the Deployment as failed, but it won\u2019t roll back automatically.</p> <p>Step 3: Verify That Kubernetes Does Not Roll Back Check the rollout history: <pre><code>kubectl rollout history deployment/nginx-deployment\n</code></pre> You'll see that Kubernetes has not reverted to the previous working version.</p> <p>Check the deployment status: <pre><code>kubectl get deployment nginx-deployment -o jsonpath='{.status.conditions}'\n</code></pre> The output will show a ProgressDeadlineExceeded condition but no rollback.</p>"},{"location":"workload_myths/Kubernetes_automatically_roll_back_failed_deployment/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Kubernetes does not automatically roll back a failed Deployment.</li> <li>You must manually trigger a rollback if needed.</li> <li>progressDeadlineSeconds only marks a deployment as failed but does not roll it back.</li> <li>CI/CD tools can be configured to handle rollback logic.</li> </ul>"},{"location":"workload_myths/Rolling_Updates_Are_Only_Supported_by_Deployments/","title":"Myth: Rolling Updates Are Only Supported by Deployments","text":"<p>Many believe that rolling updates are an exclusive feature of Kubernetes Deployments. This assumption leads to the misconception that StatefulSets and DaemonSets do not support rolling updates, forcing teams to use workarounds. But is this really the case?</p>"},{"location":"workload_myths/Rolling_Updates_Are_Only_Supported_by_Deployments/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>Deployments are widely documented for rolling updates, making them the most well-known approach.</li> <li>Lack of awareness about update strategies in other controllers like StatefulSets and DaemonSets.</li> <li>Confusion around update behavior\u2014each controller has a different approach to handling updates.</li> </ol>"},{"location":"workload_myths/Rolling_Updates_Are_Only_Supported_by_Deployments/#the-reality","title":"The Reality:","text":"<p>Rolling Updates Extend Beyond Deployments. While Deployments use the default RollingUpdate strategy, StatefulSets and DaemonSets also support rolling updates\u2014but with different behaviors. - Deployments: Roll updates across Pods gradually using RollingUpdate strategy. - StatefulSets: Follow a rolling update pattern but update one Pod at a time in order. - DaemonSets: Perform rolling updates but have different scheduling constraints, ensuring Pods are only running on specific nodes.</p>"},{"location":"workload_myths/Rolling_Updates_Are_Only_Supported_by_Deployments/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Let\u2019s look at a rolling update example for each controller.</p>"},{"location":"workload_myths/Rolling_Updates_Are_Only_Supported_by_Deployments/#todo-need-to-add-proper-example","title":"TODO - Need to add proper example","text":""},{"location":"workload_myths/Rolling_Updates_Are_Only_Supported_by_Deployments/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Rolling updates are not exclusive to Deployments\u2014StatefulSets and DaemonSets also support them.</li> <li>Each controller has a different update pattern\u2014understanding them prevents unexpected behavior.</li> <li>Kubernetes ensures updates happen safely\u2014but choosing the right approach matters!</li> </ul>"},{"location":"workload_myths/overview/","title":"Kubernetes Workload Myths","text":"<p>Deploying workloads in Kubernetes may seem straightforward\u2014but lurking underneath are subtle behaviors that often surprise even experienced engineers.</p> <p>This section dives into common misconceptions around Pods, Deployments, StatefulSets, Jobs, and DaemonSets. Misunderstanding how these controllers behave can lead to reliability issues, scaling problems, and unexpected restarts in production.</p> <p>By exposing these myths, we help you design resilient, predictable workloads that behave as intended under real-world pressure.</p>"},{"location":"workload_myths/overview/#myths","title":"Myths","text":"<ul> <li>Complete application can be rolled back in Kubernetes </li> <li>Rolling Updates Are Only Supported by Deployments </li> <li>DaemonSet always schedule pods on all nodes </li> <li>Deployment Supports All Pod Restart Policies</li> <li>Kubernetes automatically roll back failed Deployment</li> <li>Kubernetes always injects information about Services into a Pod </li> <li>All Pods are created through the API server</li> <li>Every Kubernetes Pod must have a ServiceAccount assigned</li> <li>Init containers can run in any sequence or even in parallel</li> <li>Deployments directly create and manage Pods</li> <li>Kubernetes automatically deletes older ReplicaSets created by Deployments</li> </ul>"}]}